# PyTorch 分布式训练

## DDP

1. 相关概念
(1) **group或者world**: 进程组(大部分情况下DDP的各个进程是在同一个进程组里), world内的所有process都可以互相通信;
(2) **world_size**: 总的进程数量(原则上一个p;rocess占用一个GPU是较优的);
(3) **rank**: 当前进程的序号, 用于进程之间通讯, rank=0的主机为mask节点;
(4) **local_rank**: 当前进程所对应的GPU号;
(5) **node**: 可以理解为一个server, 即机器的数量.
例如:
4台机器, 每台机器8张卡, 通过`init_process_group()`对进程组进行初始化, 然后可以通过`get_world_size()`获得到**world_size=32**, 再通过`get_rank()`获取到所有进程的编号**0~31**, 在每台机器上, **local_rank**为**0~8**.GPU之间通信的方式是**nccl**, 全称是Nvidia Collective Communication Library.
<br/>
2. 一个训练的例子
```
import torch.multiprocessing as mp 
from torch.utils.data.distributed import DistributedSampler
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.distributed import init_process_group, destroy_process_group
```

`DistributedSampler`将dataloader分发到不同的GPU上





